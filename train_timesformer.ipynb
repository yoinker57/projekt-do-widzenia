{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TimeSformer Exercise Prediction\n",
                "\n",
                "This notebook trains a TimeSformer model to predict exercises from video data.\n",
                "Data is located in `data/`, with videos in `data/films/` and labels in CSV files.\n",
                "We use `split.csv` to divide data into training and testing sets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from transformers import TimesformerForVideoClassification, VideoMAEImageProcessor, TrainingArguments, Trainer\n",
                "from transformers import AutoImageProcessor, AutoModelForVideoClassification\n",
                "import decord\n",
                "from decord import VideoReader, cpu\n",
                "from PIL import Image\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# Ensure decord works correctly\n",
                "decord.bridge.set_bridge('torch')\n",
                "import random\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_DIR = 'data'\n",
                "FILMS_DIR = os.path.join(DATA_DIR, 'films')\n",
                "SPLIT_FILE = 'split.csv'\n",
                "MODEL_CKPT = \"facebook/timesformer-base-finetuned-k400\"\n",
                "BATCH_SIZE = 4\n",
                "NUM_FRAMES = 8 # Number of frames to sample per clip\n",
                "RESIZE_TO = 224"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Preprocessing\n",
                "\n",
                "We need to parse the CSV files to identify labeled segments (clips). \n",
                "Each row in a CSV corresponds to a frame. We look for continuous segments of valid labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing 60 files...\n",
                        "Missing file for ID: 027_e84uoke1\n",
                        "Train clips: 3165, Test clips: 3179\n",
                        "Unique labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
                        "\n",
                        "Class Distribution (Train - Original):\n",
                        "Counter({1: 892, 12: 166, 11: 158, 2: 156, 4: 155, 7: 154, 14: 153, 5: 152, 10: 150, 13: 149, 16: 149, 3: 147, 6: 147, 15: 147, 8: 146, 9: 144})\n",
                        "\n",
                        "Undersampling Class 1 from 892 to 160...\n",
                        "\n",
                        "Class Distribution (Train - Balanced):\n",
                        "Counter({12: 166, 1: 160, 11: 158, 2: 156, 4: 155, 7: 154, 14: 153, 5: 152, 10: 150, 13: 149, 16: 149, 6: 147, 15: 147, 3: 147, 8: 146, 9: 144})\n",
                        "\n",
                        "Class Distribution (Test):\n",
                        "Counter({1: 906, 7: 163, 5: 157, 12: 157, 2: 155, 3: 152, 4: 152, 8: 152, 9: 152, 13: 152, 10: 150, 11: 150, 14: 147, 15: 146, 16: 145, 6: 143})\n"
                    ]
                }
            ],
            "source": [
                "train_clips, test_clips, unique_labels = prepare_dataset(SPLIT_FILE)\n",
                "print(f\"Train clips: {len(train_clips)}, Test clips: {len(test_clips)}\")\n",
                "print(f\"Unique labels: {unique_labels}\")\n",
                "\n",
                "# Class Counting (Original)\n",
                "train_labels = [c['label'] for c in train_clips]\n",
                "print(\"\\nClass Distribution (Train - Original):\")\n",
                "print(Counter(train_labels))\n",
                "\n",
                "# Balancing - Undersample Class 1 (or majority class)\n",
                "# Target -> ~160\n",
                "TARGET_COUNT = 160\n",
                "random.seed(42)\n",
                "\n",
                "# Identify class 1 clips\n",
                "class_1_clips = [c for c in train_clips if c['label'] == 1]\n",
                "other_clips = [c for c in train_clips if c['label'] != 1]\n",
                "\n",
                "if len(class_1_clips) > TARGET_COUNT:\n",
                "    print(f\"\\nUndersampling Class 1 from {len(class_1_clips)} to {TARGET_COUNT}...\")\n",
                "    class_1_clips = random.sample(class_1_clips, TARGET_COUNT)\n",
                "\n",
                "train_clips = other_clips + class_1_clips\n",
                "random.shuffle(train_clips)\n",
                "\n",
                "# Verify New Distribution\n",
                "train_labels_balanced = [c['label'] for c in train_clips]\n",
                "print(\"\\nClass Distribution (Train - Balanced):\")\n",
                "print(Counter(train_labels_balanced))\n",
                "\n",
                "test_labels = [c['label'] for c in test_clips]\n",
                "print(\"\\nClass Distribution (Test):\")\n",
                "print(Counter(test_labels))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Label Mappings: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15}\n"
                    ]
                }
            ],
            "source": [
                "# Create label mappings\n",
                "# Convert numpy types to native python types for JSON serialization\n",
                "label2id = {int(label): int(i) for i, label in enumerate(unique_labels)}\n",
                "id2label = {int(i): int(label) for label, i in label2id.items()}\n",
                "print(\"Label Mappings:\", label2id)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ExerciseDataset(Dataset):\n",
                "    def __init__(self, clips, processor, num_frames=8):\n",
                "        self.clips = clips\n",
                "        self.processor = processor\n",
                "        self.num_frames = num_frames\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.clips)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        clip = self.clips[idx]\n",
                "        video_path = clip['video_path']\n",
                "        start_f = clip['start_frame']\n",
                "        end_f = clip['end_frame']\n",
                "        label = clip['label']\n",
                "\n",
                "        # Load video\n",
                "        try:\n",
                "            vr = VideoReader(video_path, ctx=cpu(0))\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading {video_path}: {e}\")\n",
                "            return self.__getitem__((idx + 1) % len(self))\n",
                "\n",
                "        total_frames = len(vr)\n",
                "        \n",
                "        start_f = max(0, min(start_f, total_frames - 1))\n",
                "        end_f = max(0, min(end_f, total_frames - 1))\n",
                "        \n",
                "        if start_f >= end_f:\n",
                "             indices = [start_f] * self.num_frames\n",
                "        else:\n",
                "            # Sample indices evenly\n",
                "            indices = np.linspace(start_f, end_f, self.num_frames).astype(int)\n",
                "\n",
                "        video = vr.get_batch(indices)\n",
                "        if hasattr(video, 'asnumpy'):\n",
                "            video = video.asnumpy()\n",
                "        else:\n",
                "            video = video.numpy()\n",
                "        \n",
                "        # Normalize and process\n",
                "        inputs = self.processor(list(video), return_tensors=\"pt\")\n",
                "        \n",
                "        return {\n",
                "            \"pixel_values\": inputs.pixel_values.squeeze(), # (num_frames, 3, 224, 224)\n",
                "            \"labels\": torch.tensor(label2id[label])\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
                        "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([16, 768]) in the model instantiated\n",
                        "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([16]) in the model instantiated\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                }
            ],
            "source": [
                "def train_and_evaluate(num_frames):\n",
                "    print(f\"\\n=== Training with NUM_FRAMES={num_frames} ===\")\n",
                "    \n",
                "    # Processor and Model\n",
                "    # Re-init model each time to start fresh\n",
                "    processor = AutoImageProcessor.from_pretrained(MODEL_CKPT)\n",
                "    model = TimesformerForVideoClassification.from_pretrained(\n",
                "        MODEL_CKPT,\n",
                "        num_labels=len(unique_labels),\n",
                "        id2label=id2label,\n",
                "        label2id=label2id,\n",
                "        ignore_mismatched_sizes=True \n",
                "    )\n",
                "    \n",
                "    # Create datasets with specific num_frames\n",
                "    train_ds = ExerciseDataset(train_clips, processor, num_frames=num_frames)\n",
                "    test_ds = ExerciseDataset(test_clips, processor, num_frames=num_frames)\n",
                "    \n",
                "    current_output_dir = f\"timesformer_results_frames_{num_frames}\"\n",
                "    args = TrainingArguments(\n",
                "        output_dir=current_output_dir,\n",
                "        remove_unused_columns=False,\n",
                "        eval_strategy=\"epoch\",\n",
                "        save_strategy=\"epoch\",\n",
                "        learning_rate=5e-5,\n",
                "        per_device_train_batch_size=BATCH_SIZE,\n",
                "        per_device_eval_batch_size=BATCH_SIZE,\n",
                "        gradient_accumulation_steps=4, \n",
                "        warmup_ratio=0.1,\n",
                "        logging_steps=10,\n",
                "        load_best_model_at_end=True,\n",
                "        metric_for_best_model=\"accuracy\",\n",
                "        num_train_epochs=3,\n",
                "        fp16=True if torch.cuda.is_available() else False,\n",
                "        report_to=\"none\",\n",
                "        disable_tqdm=False,\n",
                "    )\n",
                "    \n",
                "    def compute_metrics(eval_pred):\n",
                "        predictions, labels = eval_pred\n",
                "        predictions = np.argmax(predictions, axis=1)\n",
                "        # Use macro F1 for multi-class balance awareness during training? Or just accuracy.\n",
                "        # Keeping accuracy for selection, but will report F1 later.\n",
                "        return {\"accuracy\": accuracy_score(labels, predictions)}\n",
                "    \n",
                "    trainer = Trainer(\n",
                "        model=model,\n",
                "        args=args,\n",
                "        train_dataset=train_ds,\n",
                "        eval_dataset=test_ds,\n",
                "        compute_metrics=compute_metrics,\n",
                "    )\n",
                "    \n",
                "    trainer.train()\n",
                "    \n",
                "    # Evaluate to get final accuracy\n",
                "    metrics = trainer.evaluate()\n",
                "    accuracy = metrics['eval_accuracy']\n",
                "    return accuracy, trainer, model\n",
                "\n",
                "# Binary Search / Ternary Search Strategy\n",
                "def search_optimal_frames(low, high):\n",
                "    print(f\"Starting Search in range [{low}, {high}]\")\n",
                "    results = {}\n",
                "    \n",
                "    def get_score(n):\n",
                "        if n in results:\n",
                "            return results[n][0]\n",
                "        acc, tr, md = train_and_evaluate(n)\n",
                "        results[n] = (acc, tr, md)\n",
                "        return acc\n",
                "    \n",
                "    l, h = low, high\n",
                "    \n",
                "    # We iterate until range is small\n",
                "    best_n = l\n",
                "    best_val = -1.0\n",
                "    \n",
                "    # Initial bounds\n",
                "    score_l = get_score(l)\n",
                "    score_h = get_score(h)\n",
                "    \n",
                "    if score_l > best_val:\n",
                "        best_val = score_l\n",
                "        best_n = l\n",
                "    if score_h > best_val:\n",
                "        best_val = score_h\n",
                "        best_n = h\n",
                "        \n",
                "    while (h - l) > 1:\n",
                "        mid = (l + h) // 2\n",
                "        if mid == l or mid == h: break\n",
                "        \n",
                "        score_m = get_score(mid)\n",
                "        \n",
                "        if score_m > best_val:\n",
                "            best_val = score_m\n",
                "            best_n = mid\n",
                "            \n",
                "        # Decision logic: Move towards the higher side\n",
                "        if score_l < score_h:\n",
                "            # Right side seems promising\n",
                "            l = mid\n",
                "            score_l = score_m\n",
                "        else:\n",
                "            # Left side seems promising\n",
                "            h = mid\n",
                "            score_h = score_m\n",
                "            \n",
                "    return best_n, results[best_n][1]\n",
                "\n",
                "# Run the search\n",
                "# Using range [4, 16] as a reasonable window around the user's 4,8,12 suggestion\n",
                "best_frames, trainer = search_optimal_frames(4, 12)\n",
                "print(f\"\\nSearch Complete. Best Frames: {best_frames}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting training...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='594' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [594/594 33:12, Epoch 3/3]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Epoch</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Accuracy</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>0.035100</td>\n",
                            "      <td>0.230442</td>\n",
                            "      <td>0.944322</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>0.016400</td>\n",
                            "      <td>0.183605</td>\n",
                            "      <td>0.958792</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>0.001100</td>\n",
                            "      <td>0.177067</td>\n",
                            "      <td>0.960994</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "TrainOutput(global_step=594, training_loss=0.20587469969785174, metrics={'train_runtime': 1995.768, 'train_samples_per_second': 4.758, 'train_steps_per_second': 0.298, 'total_flos': 8.319757294526792e+18, 'train_loss': 0.20587469969785174, 'epoch': 3.0})"
                        ]
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.save_model(f\"./models/best_model_frames_{best_frames}\")\n",
                "\n",
                "# Final Evaluation Metrics\n",
                "test_ds = ExerciseDataset(test_clips, processor, num_frames=best_frames)\n",
                "preds_output = trainer.predict(test_ds)\n",
                "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
                "y_true = preds_output.label_ids\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_true, y_preds, target_names=[str(l) for l in unique_labels]))\n",
                "\n",
                "print(\"Confusion Matrix:\")\n",
                "print(confusion_matrix(y_true, y_preds))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model wczytany pomyślnie!\n"
                    ]
                }
            ],
            "source": [
                "test_model = AutoModelForVideoClassification.from_pretrained(\"./models/model1\")\n",
                "print(\"Model wczytany pomyślnie!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
